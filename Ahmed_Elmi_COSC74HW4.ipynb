{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "575e209f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categorical Multinomial NB -> Accuracy: 0.85, F1 Score: 0.80\n",
      "Gaussian NB -> Accuracy: 0.60, F1 Score: 0.31\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "# ---------------------------\n",
    "# Read the CSV file \n",
    "data_frame = pd.read_csv('hw4_naive.csv')\n",
    "\n",
    "# ---------------------------\n",
    "# I will prepare Two Versions of the Data\n",
    "# ---------------------------\n",
    "# For Gaussian NB, we need the original numeric features\n",
    "data_numeric = data_frame.copy()  # keeps numeric values intact\n",
    "\n",
    "# For Multinomial NB (categorical version), convert feature columns to strings\n",
    "data_categorical = data_frame.copy()\n",
    "for col in data_categorical.columns[:-1]:  # ('Label')\n",
    "    data_categorical[col] = data_categorical[col].astype(str)\n",
    "\n",
    "# ---------------------------\n",
    "# Split Data for the Gaussian NB (Numeric)\n",
    "# ---------------------------\n",
    "# Select numeric features (first 6 columns) and labels\n",
    "X_num = data_numeric.iloc[:, :6].values\n",
    "y_num = data_numeric['Label'].values\n",
    "\n",
    "# Perform an 80/20 train-test split for numeric data\n",
    "X_train_num, X_test_num, y_train_num, y_test_num = train_test_split(\n",
    "    X_num, y_num, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# ---------------------------\n",
    "# Split Data for the Categorical Multinomial NB\n",
    "# ---------------------------\n",
    "# Select categorical features (first 6 columns) and labels\n",
    "\n",
    "X_cat = data_categorical.iloc[:, :6].values\n",
    "y_cat = data_categorical['Label'].values\n",
    "\n",
    "# Perform an 80/20 train-test split for categorical data\n",
    "X_train_cat, X_test_cat, y_train_cat, y_test_cat = train_test_split(\n",
    "    X_cat, y_cat, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# ---------------------------\n",
    "# Define the Categorical Multinomial NB Classifier\n",
    "\n",
    "class CategoricalMultinomialNB:\n",
    "    def __init__(self, smoothing=1.0):\n",
    "        self.smoothing = smoothing\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        # Get the unique class labels\n",
    "        self.classes = np.unique(y)\n",
    "        num_features = X.shape[1]\n",
    "        \n",
    "        # Compute vocabulary sizes for each feature \n",
    "        self.vocab_sizes_dict = {j: len(np.unique(X[:, j])) for j in range(num_features)}\n",
    "        \n",
    "        # Initialize dictionaries to count feature values per class and overall class counts\n",
    "        self.value_count_per_feature = {cls: {j: {} for j in range(num_features)} for cls in self.classes}\n",
    "        self.class_counts = {cls: 0 for cls in self.classes}\n",
    "        \n",
    "        # Count occurrences: For every sample, update the count for each feature's value under its class\n",
    "        for i in range(X.shape[0]):\n",
    "            current_class = y[i]\n",
    "            self.class_counts[current_class] += 1\n",
    "            for j in range(num_features):\n",
    "                current_value = X[i, j]\n",
    "                self.value_count_per_feature[current_class][j][current_value] = \\\n",
    "                    self.value_count_per_feature[current_class][j].get(current_value, 0) + 1\n",
    "        \n",
    "        # Compute smoothed likelihoods for each feature value in each class\n",
    "        self.likelihoods = {cls: {j: {} for j in range(num_features)} for cls in self.classes}\n",
    "        for cls in self.classes:\n",
    "            for j in range(num_features):\n",
    "                total_count_for_feature = sum(self.value_count_per_feature[cls][j].values())\n",
    "                vocab_size = self.vocab_sizes_dict[j]\n",
    "                for val, count in self.value_count_per_feature[cls][j].items():\n",
    "                    self.likelihoods[cls][j][val] = (count + self.smoothing) / \\\n",
    "                        (total_count_for_feature + self.smoothing * vocab_size)\n",
    "        \n",
    "        # Calculate class prior probabilities\n",
    "        total_samples = X.shape[0]\n",
    "        self.prior_probabilities = {cls: self.class_counts[cls] / total_samples for cls in self.classes}\n",
    "        \n",
    "    def predict(self, X):\n",
    "        predictions = []\n",
    "        num_features = X.shape[1]\n",
    "        # For each test sample, calculate the log-probability for every class\n",
    "        for i in range(X.shape[0]):\n",
    "            class_log_probs = {}\n",
    "            for cls in self.classes:\n",
    "                # Start with the log prior probability\n",
    "                log_prob = np.log(self.prior_probabilities[cls])\n",
    "                # For each feature, add the log likelihood (using smoothing if necessary)\n",
    "                for j in range(num_features):\n",
    "                    sample_val = X[i, j]\n",
    "                    total_count = sum(self.value_count_per_feature[cls][j].values())\n",
    "                    vocab_size = self.vocab_sizes_dict[j]\n",
    "                    # If the value hasn't been seen during training, use the smoothed probability\n",
    "                    prob_val = self.likelihoods[cls][j].get(\n",
    "                        sample_val,\n",
    "                        self.smoothing / (total_count + self.smoothing * vocab_size)\n",
    "                    )\n",
    "                    log_prob += np.log(prob_val)\n",
    "                class_log_probs[cls] = log_prob\n",
    "            # Choose the class with the highest log-probability as the prediction\n",
    "            predictions.append(max(class_log_probs, key=class_log_probs.get))\n",
    "        return np.array(predictions)\n",
    "\n",
    "# ---------------------------\n",
    "# Define the Gaussian NB Classifier\n",
    "\n",
    "class CustomGaussianNB:\n",
    "    def fit(self, X, y):\n",
    "        self.classes = np.unique(y)\n",
    "        self.prior_probabilities = {}\n",
    "        self.feature_means = {}\n",
    "        self.feature_variances = {}\n",
    "        \n",
    "        # For each class, compute the mean and variance for each feature\n",
    "        for cls in self.classes:\n",
    "            X_cls = X[y == cls]\n",
    "            self.prior_probabilities[cls] = X_cls.shape[0] / X.shape[0]\n",
    "            self.feature_means[cls] = np.mean(X_cls, axis=0)\n",
    "            self.feature_variances[cls] = np.var(X_cls, axis=0)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        predictions = []\n",
    "        # For each sample, compute the log-probability under each class\n",
    "        for sample in X:\n",
    "            log_probs = {}\n",
    "            for cls in self.classes:\n",
    "                # Calculate the log likelihood assuming a Gaussian distribution for each feature\n",
    "                log_likelihood = -0.5 * np.sum(np.log(2 * np.pi * self.feature_variances[cls]))\n",
    "                log_likelihood -= np.sum(((sample - self.feature_means[cls])**2) / (2 * self.feature_variances[cls]))\n",
    "                log_probs[cls] = np.log(self.prior_probabilities[cls]) + log_likelihood\n",
    "            # Predict the class with the maximum log-probability\n",
    "            predictions.append(max(log_probs, key=log_probs.get))\n",
    "        return np.array(predictions)\n",
    "\n",
    "# ---------------------------\n",
    "# Model Evaluation\n",
    "\n",
    "\n",
    "\n",
    "# Evaluate the Categorical Multinomial NB classifier\n",
    "cat_model = CategoricalMultinomialNB(smoothing=1)\n",
    "cat_model.fit(X_train_cat, y_train_cat)\n",
    "cat_predictions = cat_model.predict(X_test_cat)\n",
    "cat_accuracy = accuracy_score(y_test_cat.astype(int), cat_predictions.astype(int))\n",
    "cat_f1 = f1_score(y_test_cat.astype(int), cat_predictions.astype(int))\n",
    "print(\"Categorical Multinomial NB -> Accuracy: {:.2f}, F1 Score: {:.2f}\".format(cat_accuracy, cat_f1))\n",
    "\n",
    "# Evaluate the Gaussian NB classifier on numeric data\n",
    "gauss_model = CustomGaussianNB()\n",
    "gauss_model.fit(X_train_num, y_train_num)\n",
    "gauss_predictions = gauss_model.predict(X_test_num)\n",
    "gauss_accuracy = accuracy_score(y_test_num, gauss_predictions)\n",
    "gauss_f1 = f1_score(y_test_num, gauss_predictions)\n",
    "print(\"Gaussian NB -> Accuracy: {:.2f}, F1 Score: {:.2f}\".format(gauss_accuracy, gauss_f1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "3d9c4e4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final Clusters for K=5:\n",
      "  Cluster 0: 6 points\n",
      "  Cluster 1: 8 points\n",
      "  Cluster 2: 7 points\n",
      "  Cluster 3: 10 points\n",
      "  Cluster 4: 9 points\n",
      "Silhouette Score for k=5 (Random Seed Initialization, max_iter=50): 0.5270\n",
      "\n",
      "Final Clusters for K=2:\n",
      "  Cluster 0: 30 points\n",
      "  Cluster 1: 10 points\n",
      "For k = 2: Silhouette Score = 0.7689\n",
      "\n",
      "Final Clusters for K=3:\n",
      "  Cluster 0: 10 points\n",
      "  Cluster 1: 20 points\n",
      "  Cluster 2: 10 points\n",
      "For k = 3: Silhouette Score = 0.7192\n",
      "\n",
      "Final Clusters for K=4:\n",
      "  Cluster 0: 10 points\n",
      "  Cluster 1: 10 points\n",
      "  Cluster 2: 10 points\n",
      "  Cluster 3: 10 points\n",
      "For k = 4: Silhouette Score = 0.6294\n",
      "\n",
      "Final Clusters for K=5:\n",
      "  Cluster 0: 8 points\n",
      "  Cluster 1: 10 points\n",
      "  Cluster 2: 2 points\n",
      "  Cluster 3: 10 points\n",
      "  Cluster 4: 10 points\n",
      "For k = 5: Silhouette Score = 0.6073\n",
      "\n",
      "The best value of k based on the silhouette score is: 2\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import pairwise_distances\n",
    "import random\n",
    "\n",
    "# ---------------------------\n",
    "# First I will load the Cluster Data\n",
    "# ---------------------------\n",
    "cluster_df = pd.read_csv('hw4_cluster.csv')\n",
    "# Convert the data to a NumPy array\n",
    "data_points = cluster_df.values  # shape: (40, 2)\n",
    "\n",
    "# ---------------------------\n",
    "# Custom K-means Clustering Function\n",
    "def kmeans_custom(data, num_clusters, max_iterations=100, init_method='random_seed', centroid_method='mean'):\n",
    "    \"\"\"\n",
    "    Parameters are:\n",
    "      data           : np.array, data points.\n",
    "      num_clusters   : int, number of clusters (K).\n",
    "      max_iterations : int, maximum number of iterations.\n",
    "      init_method    : str, either 'random_seed' or 'random_split' for initialization.\n",
    "      centroid_method: str, method for centroid calculation (currently only 'mean' is supported).\n",
    "      \n",
    "    Returns:\n",
    "      clusters       : list of np.arrays, each array contains the data points (rows) that belong to that cluster.\n",
    "    \"\"\"\n",
    "    n_samples, n_features = data.shape\n",
    "\n",
    "    # Initialize centroids\n",
    "    if init_method == 'random_seed':\n",
    "        # Randomly select 'num_clusters' points from data as initial centroids.\n",
    "        initial_indices = np.random.choice(n_samples, num_clusters, replace=False)\n",
    "        centroids = data[initial_indices]\n",
    "    elif init_method == 'random_split':\n",
    "        # Shuffle the data and split it into 'num_clusters' groups,\n",
    "        # then compute the mean of each group as the initial centroid.\n",
    "        shuffled_indices = np.random.permutation(n_samples)\n",
    "        splits = np.array_split(shuffled_indices, num_clusters)\n",
    "        centroids = np.array([data[split].mean(axis=0) for split in splits])\n",
    "    else:\n",
    "        raise ValueError(\"Unknown initialization method: choose 'random_seed' or 'random_split'.\")\n",
    "\n",
    "    # For stopping criteria\n",
    "    prev_assignments = None\n",
    "\n",
    "    for it in range(max_iterations):\n",
    "        # Assignment step: assign each point to the closest centroid.\n",
    "        cluster_assignments = []\n",
    "        for point in data:\n",
    "            # Compute Euclidean distances from the point to each centroid.\n",
    "            distances = np.linalg.norm(point - centroids, axis=1)\n",
    "            # Assign the point to the cluster with the minimum distance.\n",
    "            cluster_assignments.append(np.argmin(distances))\n",
    "        cluster_assignments = np.array(cluster_assignments)\n",
    "\n",
    "        # Stopping condition: if assignments haven't changed, break early.\n",
    "        if prev_assignments is not None and np.array_equal(cluster_assignments, prev_assignments):\n",
    "            # No change in clusters -> convergence achieved.\n",
    "            break\n",
    "        prev_assignments = cluster_assignments.copy()\n",
    "\n",
    "        # Update step: recalculate centroids for each cluster.\n",
    "        new_centroids = np.zeros((num_clusters, n_features))\n",
    "        for k in range(num_clusters):\n",
    "            cluster_points = data[cluster_assignments == k]\n",
    "            if len(cluster_points) > 0:\n",
    "                # Using mean as the centroid.\n",
    "                new_centroids[k] = cluster_points.mean(axis=0)\n",
    "            else:\n",
    "                # If a cluster gets no points, reinitialize its centroid randomly.\n",
    "                new_centroids[k] = data[np.random.choice(n_samples)]\n",
    "        centroids = new_centroids\n",
    "\n",
    "    # Create list of clusters (each cluster is an array of points)\n",
    "    clusters = [data[cluster_assignments == k] for k in range(num_clusters)]\n",
    "    \n",
    "    # Print cluster sizes \n",
    "    print(f\"\\nFinal Clusters for K={num_clusters}:\")\n",
    "    for idx, cluster_data in enumerate(clusters):\n",
    "        print(f\"  Cluster {idx}: {len(cluster_data)} points\")\n",
    "        \n",
    "        \n",
    "    return clusters\n",
    "\n",
    "\n",
    "# Silhouette Score Calculation Function\n",
    "\n",
    "def calculate_silhouette_score(clusters):\n",
    "    \"\"\"\n",
    "    Calculates the silhouette score for a clustering result.\n",
    "    \n",
    "    Parameters:\n",
    "      clusters : list of np.arrays, where each array contains the data points (rows) in that cluster.\n",
    "    \n",
    "    Returns:\n",
    "      silhouette_avg : float, the average silhouette score over all data points.\n",
    "    \"\"\"\n",
    "    # Combine all clusters into one data array and get corresponding cluster labels.\n",
    "    all_points = np.vstack(clusters)\n",
    "    labels = []\n",
    "    for cluster_index, cluster in enumerate(clusters):\n",
    "        labels.extend([cluster_index] * len(cluster))\n",
    "    labels = np.array(labels)\n",
    "    \n",
    "    # Number of samples\n",
    "    n = all_points.shape[0]\n",
    "    if n == 0:\n",
    "        return 0\n",
    "\n",
    "    # Precompute pairwise distances between all points\n",
    "    distance_matrix = pairwise_distances(all_points, metric='euclidean')\n",
    "    \n",
    "    silhouette_scores = []\n",
    "    for i in range(n):\n",
    "        own_cluster = labels[i]\n",
    "        # a(i): average distance to all other points in the same cluster.\n",
    "        same_cluster = np.where(labels == own_cluster)[0]\n",
    "        if len(same_cluster) > 1:\n",
    "            a_i = np.mean(distance_matrix[i, same_cluster][distance_matrix[i, same_cluster] != 0])\n",
    "        else:\n",
    "            a_i = 0 \n",
    "        \n",
    "        b_i = np.inf\n",
    "        for cluster_id in np.unique(labels):\n",
    "            if cluster_id == own_cluster:\n",
    "                continue\n",
    "            other_cluster = np.where(labels == cluster_id)[0]\n",
    "            b_i = min(b_i, np.mean(distance_matrix[i, other_cluster]))\n",
    "        \n",
    "        # Silhouette score for the point.\n",
    "        if max(a_i, b_i) > 0:\n",
    "            s_i = (b_i - a_i) / max(a_i, b_i)\n",
    "        else:\n",
    "            s_i = 0\n",
    "        silhouette_scores.append(s_i)\n",
    "    \n",
    "    # Return the average silhouette score.\n",
    "    silhouette_avg = np.mean(silhouette_scores)\n",
    "    return silhouette_avg\n",
    "\n",
    "# ---------------------------\n",
    "# Test K-means and Silhouette Score\n",
    "\n",
    "# For the test: k=5, init_method = 'random_seed', max_iter = 50, centroid method = mean.\n",
    "clusters_for_k5 = kmeans_custom(data_points, num_clusters=5, max_iterations=50, init_method='random_seed', centroid_method='mean')\n",
    "silhouette_k5 = calculate_silhouette_score(clusters_for_k5)\n",
    "print(\"Silhouette Score for k=5 (Random Seed Initialization, max_iter=50): {:.4f}\".format(silhouette_k5))\n",
    "\n",
    "# ---------------------------\n",
    "# Extra Credit : Finding the Best K\n",
    "# Run K-means for k = 2, 3, 4, 5 using:\n",
    "# - centroid method: mean (default)\n",
    "# - initialization method: 'random_split'\n",
    "# - max_iterations: 100\n",
    "k_values = [2, 3, 4, 5]\n",
    "silhouette_scores = {}\n",
    "\n",
    "for k in k_values:\n",
    "    clusters_k = kmeans_custom(data_points, num_clusters=k, max_iterations=100, init_method='random_split', centroid_method='mean')\n",
    "    score = calculate_silhouette_score(clusters_k)\n",
    "    silhouette_scores[k] = score\n",
    "    print(\"For k = {}: Silhouette Score = {:.4f}\".format(k, score))\n",
    "\n",
    "# Determine the best k\n",
    "best_k = max(silhouette_scores, key=silhouette_scores.get)\n",
    "print(\"\\nThe best value of k based on the silhouette score is:\", best_k)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48db5e7d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de8fe901",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
